{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTRUCTIONS\n",
    "\n",
    "* Make sure to run all the cells before trying to train the model.\n",
    "* See the \"Network code\" section for a description of what each argument is\n",
    "    for the neural network. \n",
    "* Everything you need is in the \"Set hyperparameters, train model, see results\" \n",
    "    section: all the hyperparameters to play with\n",
    "* Let me know if something isn't working or if you have questions\n",
    "* USE PYTHON 3\n",
    "\n",
    "\n",
    "* **IMPORTANT** As you're training the model, it will show you how the training and validation\n",
    "    RMSEs change over time. If you see the model isn't improving anymore, just hit \n",
    "    \"Kernel\" -> \"Interrupt Kernel\" at the bar up top. The training method catches \n",
    "    the exception and will return just fine. \n",
    "* Also try running the \"Side experiment\" section if u want\n",
    "\n",
    "* **TIPS**: \n",
    "    * if the validation/training RMSEs are fluctuating a lot, lower the learning rate. It needs to be pretty low for the learning dynamic to be stable\n",
    "    * Try varying the number of hidden units per layer; that seems to work pretty well for me\n",
    "    * Be suspicious if the training RMSE is much lower than the validation RMSE - if you make the network huge (without enough regularization), it can \"memorize\" the training set and will do horribly on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.get_data import *\n",
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import IntProgress\n",
    "from collections import OrderedDict\n",
    "import warnings\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Feature 'attributes_AcceptsInsurance' ==========\n",
      "TYPE: boolean. Changing False -> 0, True -> 1.\n",
      "Detected NaN in column. Replacing with mean of non-NaN values.\n",
      "\n",
      "========== Feature 'attributes_AgesAllowed' ==========\n",
      "TYPE: string. Doing one-hot encoding.\n",
      "\n",
      "========== Feature 'attributes_Alcohol' ==========\n",
      "TYPE: string. Doing one-hot encoding.\n",
      "\n",
      "========== Feature 'attributes_Ambience' ==========\n",
      "TYPE: dict. Creating new features and doing one-hot encoding.\n",
      "\n",
      "========== Feature 'attributes_BusinessParking' ==========\n",
      "TYPE: dict. Creating new features and doing one-hot encoding.\n",
      "\n",
      "========== Feature 'attributes_Caters' ==========\n",
      "TYPE: boolean. Changing False -> 0, True -> 1.\n",
      "Detected NaN in column. Replacing with mean of non-NaN values.\n",
      "\n",
      "========== Feature 'attributes_DogsAllowed' ==========\n",
      "TYPE: boolean. Changing False -> 0, True -> 1.\n",
      "Detected NaN in column. Replacing with mean of non-NaN values.\n",
      "\n",
      "========== Feature 'attributes_GoodForDancing' ==========\n",
      "TYPE: boolean. Changing False -> 0, True -> 1.\n",
      "Detected NaN in column. Replacing with mean of non-NaN values.\n",
      "\n",
      "========== Feature 'attributes_GoodForKids' ==========\n",
      "TYPE: boolean. Changing False -> 0, True -> 1.\n",
      "Detected NaN in column. Replacing with mean of non-NaN values.\n",
      "\n",
      "========== Feature 'attributes_GoodForMeal' ==========\n",
      "TYPE: dict. Creating new features and doing one-hot encoding.\n",
      "\n",
      "========== Feature 'attributes_HasTV' ==========\n",
      "TYPE: boolean. Changing False -> 0, True -> 1.\n",
      "Detected NaN in column. Replacing with mean of non-NaN values.\n",
      "\n",
      "========== Feature 'attributes_NoiseLevel' ==========\n",
      "TYPE: string. Doing one-hot encoding.\n",
      "\n",
      "========== Feature 'attributes_OutdoorSeating' ==========\n",
      "TYPE: boolean. Changing False -> 0, True -> 1.\n",
      "Detected NaN in column. Replacing with mean of non-NaN values.\n",
      "\n",
      "========== Feature 'attributes_RestaurantsAttire' ==========\n",
      "TYPE: string. Doing one-hot encoding.\n",
      "\n",
      "========== Feature 'attributes_RestaurantsDelivery' ==========\n",
      "TYPE: boolean. Changing False -> 0, True -> 1.\n",
      "Detected NaN in column. Replacing with mean of non-NaN values.\n",
      "\n",
      "========== Feature 'attributes_RestaurantsGoodForGroups' ==========\n",
      "TYPE: boolean. Changing False -> 0, True -> 1.\n",
      "Detected NaN in column. Replacing with mean of non-NaN values.\n",
      "\n",
      "========== Feature 'attributes_RestaurantsPriceRange2' ==========\n",
      "TYPE: numeric.\n",
      "Detected NaN in column. Replacing with mean of non-NaN values.\n",
      "\n",
      "========== Feature 'attributes_RestaurantsReservations' ==========\n",
      "TYPE: boolean. Changing False -> 0, True -> 1.\n",
      "Detected NaN in column. Replacing with mean of non-NaN values.\n",
      "\n",
      "========== Feature 'attributes_RestaurantsTableService' ==========\n",
      "TYPE: boolean. Changing False -> 0, True -> 1.\n",
      "Detected NaN in column. Replacing with mean of non-NaN values.\n",
      "\n",
      "========== Feature 'attributes_WiFi' ==========\n",
      "TYPE: string. Doing one-hot encoding.\n",
      "\n",
      "========== Feature 'latitude' ==========\n",
      "TYPE: numeric.\n",
      "\n",
      "========== Feature 'longitude' ==========\n",
      "TYPE: numeric.\n",
      "\n",
      "========== Feature 'review_count' ==========\n",
      "TYPE: numeric.\n",
      "\n",
      "Constructing design matrix now.\n",
      "0/150232 done\n",
      "20000/150232 done\n",
      "40000/150232 done\n",
      "60000/150232 done\n",
      "80000/150232 done\n",
      "100000/150232 done\n",
      "120000/150232 done\n",
      "140000/150232 done\n",
      "Finished!\n",
      "Constructing design matrix now.\n",
      "0/50077 done\n",
      "20000/50077 done\n",
      "40000/50077 done\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "b_data, u_data, train_reviews = get_training_data(b_cols='maybe', verbose=True)\n",
    "valid_reviews = get_validation_reviews()\n",
    "\n",
    "# due to quirks with how PyTorch works, I have to subtract one from the stars, \n",
    "#   so that they range from [0, 4]\n",
    "train_reviews['stars'] -= 1\n",
    "valid_reviews['stars'] -= 1\n",
    "\n",
    "X_train, y_train = construct_design_matrix(\n",
    "    b_data, u_data, train_reviews, return_df=False, verbose=True\n",
    ")\n",
    "X_valid, y_valid = construct_design_matrix(\n",
    "    b_data, u_data, valid_reviews, return_df=False, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/50078 done\n",
      "20000/50078 done\n",
      "40000/50078 done\n"
     ]
    }
   ],
   "source": [
    "test_reviews = pd.read_csv('./data/test_queries.csv')\n",
    "N = len(test_reviews['user_id'])\n",
    "Db = len(b_data.columns)\n",
    "Du = len(u_data.columns)\n",
    "D = Db + Du\n",
    "\n",
    "all_cols = np.append(b_data.columns, u_data.columns)\n",
    "X_test = np.zeros((N, D))\n",
    "\n",
    "for i, review in test_reviews.iterrows():\n",
    "    if (i % 20000) == 0:\n",
    "        print('%d/%d done' % (i, N))\n",
    "        \n",
    "    u_id = review['user_id']\n",
    "    b_id = review['business_id']\n",
    "    \n",
    "    X_test[i, :Db] = b_data.loc[b_id].values\n",
    "    X_test[i, Db:] = u_data.loc[u_id].values\n",
    "    \n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Network code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     37
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed forward neural network.\n",
    "    \n",
    "    === PARAMETERS ===============================================================\n",
    "    \n",
    "    (int) in_size: number of input features\n",
    "    \n",
    "    (int) num_hidden_layers: number of hidden layers.\n",
    "    \n",
    "    (int or list/np.array) hidden_size: number of hidden units.\n",
    "                * if hidden_size=int, then each hidden layer has that many units.\n",
    "                * if it is a list/array, then it specifies the number of hidden\n",
    "                    units for EACH layer.\n",
    "                                \n",
    "    (string) model_type: \"softmax\" or \"regression\"\n",
    "                                \n",
    "    (int) out_size: number of output units (5 for softmax, 1 for regression)\n",
    "    \n",
    "    (bool) do_batchnorm: have not implemented this, ignore it\n",
    "    \n",
    "    (bool) do_dropout: do dropout training (adds dropout layers). You can ignore this\n",
    "            if you want, it makes training tricky\n",
    "            \n",
    "    (float) p: dropout probability\n",
    "    \n",
    "    =================================================================================\n",
    "    \n",
    "    \n",
    "    TODO: batchnorm (maybe not tho)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, in_size, hidden_size, num_hidden_layers, out_size, activation,\n",
    "        model_type, do_batchnorm=True, do_dropout=True, p=0.3\n",
    "    ):\n",
    "        super(FFNN, self).__init__()\n",
    "        self.model_type = model_type\n",
    "        msg = 'Unknown model type %s' % model_type\n",
    "        assert model_type in ['softmax', 'regression'], msg\n",
    "        \n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        if type(hidden_size) is int:\n",
    "            self.hidden_size = [hidden_size]*num_hidden_layers\n",
    "        elif type(hidden_size) in [list, np.ndarray]:\n",
    "            msg = \"num_hidden_layers != len(hidden_size) \" \n",
    "            assert len(hidden_size) == num_hidden_layers, msg\n",
    "        else:\n",
    "            msg = \"OOPS: hidden_size should be an int, list, or np.array\"\n",
    "            msg = msg + '\\nhidden_size = %s ?' % hidden_size\n",
    "            raise ValueError(msg)\n",
    "        \n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        if do_dropout:\n",
    "            self.layers['dropout_in'] = nn.Dropout(p=0.15)\n",
    "        self.layers['linear_in'] = nn.Linear(in_size, hidden_size[0])\n",
    "        self.layers['activ_in'] = activation()\n",
    "        \n",
    "        for i in range(1, num_hidden_layers):\n",
    "            if do_dropout:\n",
    "                self.layers['dropout_%d' % i] = nn.Dropout(p=p)\n",
    "            self.layers['linear_%d' % i] = nn.Linear(\n",
    "                hidden_size[i-1], hidden_size[i]\n",
    "            )\n",
    "            self.layers['activ_%d' % i] = activation()\n",
    "            \n",
    "        self.layers['linear_out'] = nn.Linear(hidden_size[-1], out_size)\n",
    "        \n",
    "        self.model = nn.Sequential(self.layers)\n",
    "        \n",
    "    def forward(self, x):    \n",
    "        return self.model(x)\n",
    "    \n",
    "    def predict(self, x, return_probs=False):\n",
    "        \"\"\"\n",
    "        Returns class predictions for each data point in x - a numpy array of shape\n",
    "            (len(x), ).\n",
    "        \n",
    "        If return_probs=True, returns class probabilities instead - as a numpy array \n",
    "            of size ( len(x), num_classes ).\n",
    "        \"\"\"\n",
    "        N = x.shape[0]\n",
    "        \n",
    "        if isinstance(x, Variable):\n",
    "            scores = self.model(x).data.numpy()\n",
    "        elif isinstance(x, torch.Tensor):\n",
    "            scores = self.model(x).numpy()\n",
    "        else:\n",
    "            scores = self.model(torch.FloatTensor(x)).detach().numpy()\n",
    "            \n",
    "        if self.model_type == 'regression':\n",
    "            return scores.squeeze()\n",
    "        \n",
    "        try:\n",
    "            probs = np.exp(scores.astype(np.float64)) # avoid overflow errors\n",
    "        except:\n",
    "            print(scores.dtype)\n",
    "            print(scores.min(), scores.max())\n",
    "            raise\n",
    "            \n",
    "        try:\n",
    "            probs /= probs.sum(axis=1).reshape(N, 1)\n",
    "        except:\n",
    "            print(probs.shape)\n",
    "            print(np.isnan(scores).any())\n",
    "            print(np.isnan(probs.sum(axis=1)).any())\n",
    "            raise\n",
    "        \n",
    "        if return_probs:\n",
    "            return probs\n",
    "        \n",
    "        predictions = probs.argmax(axis=1)\n",
    "        return predictions\n",
    "    \n",
    "    def get_hidden_representations(self, x):\n",
    "        x = torch.Tensor(x)\n",
    "\n",
    "        reps = []\n",
    "        for name, layer in self.layers.items():\n",
    "            x = layer(x)\n",
    "            if 'activ' in name or 'out' in name:\n",
    "                reps.append(x.detach().numpy())\n",
    "                \n",
    "        \n",
    "        return reps\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0,
     26
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, X, y):\n",
    "    \"\"\" Calculate the accuracy on the FULL data X and y. \"\"\"\n",
    "    y_pred = np.zeros_like(y)\n",
    "    for i in range(0, X.shape[0], 100):\n",
    "        x_in = X[i:i+100]\n",
    "        \n",
    "        y_out = model.predict(x_in)\n",
    "        \n",
    "        y_pred[i:i+100] = y_out\n",
    "        \n",
    "    accuracy = np.mean(np.where(y_pred == y, 1, 0))\n",
    "    return accuracy\n",
    "\n",
    "def calculate_rmse(model, X, y):\n",
    "    \"\"\" Calculate the RMSE on the FULL data X and y. \"\"\"\n",
    "    y_pred = np.zeros_like(y)\n",
    "    for i in range(0, X.shape[0], 100):\n",
    "        x_in = X[i:i+100]\n",
    "        \n",
    "        y_out = model.predict(x_in)\n",
    "        \n",
    "        y_pred[i:i+100] = y_out\n",
    "        \n",
    "    rmse = np.sqrt(np.mean((y - y_pred)**2))\n",
    "    return rmse\n",
    "\n",
    "def sample_rmse(model, X, y, n_samples=1000):\n",
    "    \"\"\" \n",
    "    Calculate the RMSE on 'n_samples' randomly chosen data points from X and y. \n",
    "    \"\"\"\n",
    "    sample_idx = np.random.choice(X.shape[0], size=n_samples)\n",
    "    X_s, y_s = X[sample_idx], y[sample_idx]\n",
    "    \n",
    "    y_pred = np.zeros_like(y_s)\n",
    "    for i in range(0, X_s.shape[0], 100):\n",
    "        x_in = X_s[i:i+100]\n",
    "        \n",
    "        y_out = model.predict(x_in)\n",
    "        \n",
    "        y_pred[i:i+100] = y_out\n",
    "        \n",
    "    rmse = np.sqrt(np.mean((y_s - y_pred)**2))\n",
    "    return rmse\n",
    "\n",
    "def moving_average(values, alpha=0.25):\n",
    "    \"\"\" Exponential moving average \"\"\"\n",
    "    N = len(values)\n",
    "    if N == 1:\n",
    "        return values[0]\n",
    "    \n",
    "    weights = np.array([(1 - alpha)**i for i in range(len(values))])[::-1]\n",
    "    return np.sum(weights * values) / np.sum(weights)\n",
    "\n",
    "# class BestModels(object):\n",
    "    \n",
    "#     def __init__(self, track_how_many=5, keep_intervals=None):\n",
    "#         self.models = dict()\n",
    "#         self.track_how_many = track_how_many\n",
    "#         self.keep_intervals = keep_intervals\n",
    "        \n",
    "#     def add(self, model, score, epoch):\n",
    "        \n",
    "#         self.models.append(model)\n",
    "#         self.stats[model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train(params, model_type='softmax'):\n",
    "    # Unpack parameters\n",
    "    if True:\n",
    "        input_size = params['in_size']\n",
    "        num_epochs = params['num_epochs']\n",
    "        batch_size = params['batch_size']\n",
    "        learning_rate = params['learning_rate']\n",
    "        hidden_size = params['hidden_size']\n",
    "        num_hidden_layers = params['num_hidden_layers']\n",
    "        activation = params['activation']\n",
    "        do_dropout = params['do_dropout']\n",
    "        do_batchnorm = params['do_batchnorm'] \n",
    "        \n",
    "    if model_type == 'softmax':\n",
    "        output_size = 5\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        t_tensor = lambda y_b: torch.LongTensor(y_b).reshape(-1, )\n",
    "    elif model_type == 'regression':\n",
    "        output_size = 1\n",
    "        criterion = nn.MSELoss()\n",
    "        t_tensor = lambda y_b: torch.FloatTensor(y_b).reshape(-1, 1)\n",
    "    else:\n",
    "        raise ValueError('Unknown model type %s.' % model_type)\n",
    "\n",
    "    warnings.filterwarnings(\"error\")\n",
    "\n",
    "    model = FFNN(\n",
    "        input_size, hidden_size, num_hidden_layers, output_size, activation,\n",
    "        model_type, do_batchnorm, do_dropout\n",
    "    )\n",
    "    \n",
    "    output = dict()\n",
    "    output['log'] = []\n",
    "    output['best_model'] = None\n",
    "    output['best_rmse'] = np.inf\n",
    "\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    stats = np.zeros((num_epochs, 5))\n",
    "#     instability_measures = np.zeros(num_epochs)\n",
    "    \n",
    "    N = X_train.shape[0]\n",
    "    num_batches = int(np.ceil(N / batch_size))\n",
    "\n",
    "    epo_meter = IntProgress(min=0, max=num_epochs, description='Epoch:')\n",
    "    btc_meter = IntProgress(min=0, max=num_batches, description='Batch:')\n",
    "    display(epo_meter)\n",
    "    display(btc_meter)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            # shuffle review order\n",
    "            shuf_idx = np.random.permutation(N)\n",
    "\n",
    "            losses = []\n",
    "            for b_num in range(num_batches):\n",
    "                # Create batch\n",
    "                b_begin, b_end = b_num * batch_size, (b_num+1) * batch_size\n",
    "                batch_idx = shuf_idx[b_begin:b_end]\n",
    "\n",
    "                X_batch, y_batch = X_train[batch_idx], y_train[batch_idx]\n",
    "                X_batch = Variable(torch.FloatTensor(X_batch))\n",
    "                y_batch = t_tensor(y_batch)\n",
    "\n",
    "                # forward pass; compute loss\n",
    "                outputs = model(X_batch)\n",
    "                try:\n",
    "                    loss = criterion(outputs, y_batch)\n",
    "                except RuntimeError:\n",
    "                    print(np.unique(y_batch.numpy()))\n",
    "                    print(epoch, b_num)\n",
    "                    raise\n",
    "\n",
    "                # Backward pass; update parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                losses.append(loss.data.item())\n",
    "\n",
    "                btc_meter.value += 1\n",
    "\n",
    "            btc_meter.value = 0\n",
    "            epo_meter.value += 1\n",
    "\n",
    "            avg_loss = np.mean(losses)\n",
    "            stats[epoch, 0] = avg_loss\n",
    "\n",
    "            model.eval()\n",
    "            try:\n",
    "                train_rmse = calculate_rmse(model, X_train, y_train)\n",
    "                stats[epoch, 1] = train_rmse\n",
    "            except RuntimeWarning:\n",
    "                message = 'Warning: NaN encountered doing train RMSE at epo %d' % epoch\n",
    "                output['log'].append(message)\n",
    "                print(message)\n",
    "                time.sleep(0.25)\n",
    "\n",
    "            valid_rmse = calculate_rmse(model, X_valid, y_valid)\n",
    "            stats[epoch, 2] = valid_rmse\n",
    "\n",
    "            if valid_rmse < output['best_rmse']:\n",
    "                output['best_rmse'] = valid_rmse\n",
    "                output['best_model'] = deepcopy(model)\n",
    "                output['best_epoch'] = epoch + 1\n",
    "                \n",
    "            print(' Valid RMSE: %f' % valid_rmse)\n",
    "            print('Best so far: %f' % output['best_rmse'])\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            clear_output()\n",
    "            display(epo_meter)\n",
    "            display(btc_meter)\n",
    "\n",
    "            f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "            ax1.set_title('Stats vs epoch')\n",
    "            ax1.plot(range(epoch+1), stats[:epoch+1, 1], label='Train RMSE')\n",
    "            ax1.plot(range(epoch+1), stats[:epoch+1, 2], label='Valid RMSE')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('RMSE')\n",
    "            ax1.legend(*ax1.get_legend_handles_labels())\n",
    "\n",
    "            if epoch > 3:\n",
    "                mn = max(0, epoch-20)\n",
    "                xs = range(mn, epoch+1)\n",
    "                recent_valid_rmses = stats[mn:epoch+1, 2]\n",
    "                z = np.polyfit(xs, recent_valid_rmses, deg=1)\n",
    "                p = np.poly1d(z)\n",
    "                ax1.plot(xs, p(xs), color='red', linestyle='--')\n",
    "\n",
    "            alpha = 0.25\n",
    "            stats[epoch, 3] = moving_average(stats[:epoch+1, 1], alpha)\n",
    "            stats[epoch, 4] = moving_average(stats[:epoch+1, 2], alpha)\n",
    "            ax2.set_title('RMSE exponential moving averages (alpha=%.3f)' % alpha)\n",
    "            ax2.set_xlabel('Epoch')\n",
    "            ax2.plot(range(epoch+1), stats[:epoch+1, 3], label='EMA of train RMSE')\n",
    "            ax2.plot(range(epoch+1), stats[:epoch+1, 4], label='EMA of valid RMSE')\n",
    "\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            instability = moving_average(\n",
    "                np.sqrt((stats[:epoch+1, 3] - stats[:epoch+1, 1])**2)\n",
    "            )\n",
    "            print('Instability measure: %f' % instability)\n",
    "            if epoch > 3:\n",
    "                print(\n",
    "                    'valid error decreasing (roughly on average) by %f/epoch ' % -z[0]\n",
    "                )\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        clear_output()\n",
    "        print('Interrupted - returning early')\n",
    "\n",
    "    epo_meter.close()\n",
    "    btc_meter.close()\n",
    "\n",
    "    model.eval() \n",
    "    warnings.filterwarnings(\"default\")\n",
    "    \n",
    "    output['model'] = model\n",
    "    output['stats'] = stats\n",
    "    \n",
    "    best_epo, best_err = output['best_epoch'], output['best_rmse']\n",
    "    print('Best valid RMSE achieved: %f at epoch %d' % (best_err, best_epo))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set hyperparameters, train model, see results\n",
    "\n",
    "# (DO STUFF HERE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     16
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted - returning early\n",
      "Best valid RMSE achieved: 1.060325 at epoch 73\n"
     ]
    }
   ],
   "source": [
    "# Fixed\n",
    "input_size = len(b_data.columns) + len(u_data.columns)\n",
    "\n",
    "# Hyperparameters / architecture choices\n",
    "num_epochs = 250       # <- need to train for a lot of epochs\n",
    "batch_size = 100\n",
    "learning_rate = 1e-4   \n",
    "do_dropout = False\n",
    "do_batchnorm = False   # ignore this, I haven't implemented it\n",
    "\n",
    "hidden_size = [50, 40, 30, 20, 10]\n",
    "num_hidden_layers = 5\n",
    "activation = nn.ReLU\n",
    "\n",
    "model_type = 'regression'  # regression seems to work better, but definitely try \n",
    "                           # softmax as well\n",
    "\n",
    "# Pack parameters into dict\n",
    "if True:\n",
    "    params = dict()\n",
    "    params['in_size'] = input_size\n",
    "    params['num_epochs'] = num_epochs\n",
    "    params['batch_size'] = batch_size\n",
    "    params['learning_rate'] = learning_rate\n",
    "    params['hidden_size'] = hidden_size\n",
    "    params['num_hidden_layers'] = num_hidden_layers\n",
    "    params['activation'] = activation\n",
    "    params['do_dropout'] = do_dropout\n",
    "    params['do_batchnorm'] = do_batchnorm\n",
    "    \n",
    "results = train(params, model_type='regression')\n",
    "model, stats = results['model'], results['stats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Side experiment - do ridge regression with neural network's hidden representations\n",
    "\n",
    "## this seems bugged tho - changing regularization strength isn't changing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reps_v = results['best_model'].get_hidden_representations(X_valid)\n",
    "reps_v = np.hstack(reps_v)\n",
    "f_rep_v = np.hstack([X_valid, reps_v])\n",
    "\n",
    "reps_t = results['best_model'].get_hidden_representations(X_train)\n",
    "reps_t = np.hstack(reps_t)\n",
    "f_rep_t = np.hstack([X_train, reps_t])\n",
    "\n",
    "alphas = [0.01, 0.1, 0.2, 0.3, 0.5, 0.8, 1.0, 2.0, 5.0, 10.0]\n",
    "valid_rmses = []\n",
    "train_rmses = []\n",
    "\n",
    "c_rmse = lambda y1, y2: np.sqrt(np.mean((y1 - y2)**2))\n",
    "\n",
    "for alpha in alphas:\n",
    "    print('Doing alpha=%f' % alpha)\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(f_rep_t, y_train)\n",
    "\n",
    "    y_pred_t = ridge.predict(f_rep_t)\n",
    "    y_pred_v = ridge.predict(f_rep_v)\n",
    "    \n",
    "    valid_rmses.append(c_rmse(y_pred_v, y_valid))\n",
    "    train_rmses.append(c_rmse(y_pred_t, y_train))\n",
    "    clear_output()\n",
    "    \n",
    "f, ax = plt.subplots(figsize=(12, 8))\n",
    "ax.plot(alphas, train_rmses, label='Train RMSEs', color='blue')\n",
    "ax.scatter(alphas, train_rmses, color='blue')\n",
    "ax.plot(alphas, valid_rmses, label='Valid RMSEs', color='orange')\n",
    "ax.scatter(alphas, valid_rmses, color='orange')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0495023843020692\n"
     ]
    }
   ],
   "source": [
    "predictor = Ridge(alpha=1.0)\n",
    "ridge.fit(f_rep_t, y_train)\n",
    "\n",
    "y_pred_v = ridge.predict(f_rep_v)\n",
    "print(c_rmse(y_pred_v, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 4.02280444],\n",
       "       [1.        , 4.49637701],\n",
       "       [2.        , 3.67470039],\n",
       "       [3.        , 3.47687721],\n",
       "       [4.        , 1.885563  ],\n",
       "       [5.        , 4.22086761],\n",
       "       [6.        , 4.28066553],\n",
       "       [7.        , 3.56951202],\n",
       "       [8.        , 4.22230486],\n",
       "       [9.        , 4.95498857]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reps_t = results['best_model'].get_hidden_representations(X_test)\n",
    "reps_t = np.hstack(reps_t)\n",
    "f_rep_t = np.hstack([X_test, reps_t])\n",
    "\n",
    "y_pred_t = ridge.predict(f_rep_t) + 1\n",
    "y_pred_t = np.clip(y_pred_t, 1, 5)\n",
    "y_pred_t = np.hstack(\n",
    "    [np.arange(len(y_pred_t), dtype=int).reshape(-1, 1), y_pred_t.reshape(-1, 1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = pd.DataFrame(data=y_pred_t, columns=['index', 'stars'])\n",
    "to_save['index'] = to_save['index'].apply(int)\n",
    "to_save.to_csv('ridgeNN_test_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
